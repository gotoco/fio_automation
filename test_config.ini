[test]
# @Mandatory field
# test_name is a part of the tag for output name files
test_name = test_pcid1kpti1

# @Mandatory field
fs2test = xfs

# @Mandatory field
blk_size = 512

# @Mandatory field
# Workload type, available values:
# read, write, randwrite, rw/readwrite, randrw, trimwrite
workload = randrw

# @Optional field
# Defines how the job issues io to the files
# See more in ioengine session in fio man pages.
ioengine = libaio

# @Optional field
# Defines how many io units to keep in  flight against
# the dile. The default is 1 for each file defined in job.
iodepth = 64

# @Optional field
# Is IO direct? (ZFS doesn't support O_DIRECT)
direct = 1

file_size = 10G
num_jobs = 1,4,8
run_time = 10

# @Mandatory field
# Define where fio will be performed
directory = /mnt
mix_read = 30

# @Mandatory field
# Interval for monitoring tools like vmstat/iostat
interval = 5

# @Optional nr of files
# If bigger than 1024 make sure that your system support that
nr_files = 1,1000

# @Optional
# In case of more option this parameter is passed directly to fio
additional_cmd = --randrepeat=1 --loops=2

# @Oprional
# Use --group_reporting=1 to combine results from all jobs
aggregate_jobs = 1

[setup]
# @Mandatory field
drives = sdb1,sdc1,sdd1,sde1
# @Optional field for ZFS
dev_name = lvm_vol
mount_root = /mnt
# @Mandatory field
# For LVM disk group name, For ZFS pool name
disk_group_name = rootdg

[setup_ext4]
mkfs = -E lazy_itable_init=0,lazy_journal_init=0

[setup_xfs]
mkfs =

[monitoring]
# @Mandatory field
# Is monitoring enabled?
monitoring = 1

# @Mandatory fields
# What tools should run during the test:
vmstat = 1
iostat = 1
mpstat = 0
sar = 0
perf = 0
# @Optional fields
# With what params?
vmstat_args = 5 --one-header
iostat_args =
mpstat_args =
sar_args =

# Additional run options Like 'perf' or 'time'.
run_with_time = True
# This will add perf before fio command,
run_with_perf = True

# Here you don't need to add perf at the begining just perf commands
perf_args = stat -e raw_syscalls:sys_enter,context-switches,page-faults

[csv_output]
## CSV output specification:
#   Based on MONITORING section extract stats from files to the rows:

# @Mandatory field
csv_header = $TAG,READ,WRITE,IOSr,IOSw,VMScs,VMSus,VMSsy,VMSid,VMSwa
# TODO: csv_header = $TAG,READ,WRITE,IOSr,IOSw,CPUu,CPUs,CPUctx,VMScs,VMSus,VMSsy,VMSid,VMSwa
# order = (0)$TAG,(1)READ,(2)WRITE,(3)IOSr,(4)IOSw,(5)CPUu,(6)CPUs,(7)CPUctx,(8)VMScs,(9)VMSus,(10)VMSsy,(11)VMSid,(12)VMSwa
#   Below (N) suffix define mapping HEADER->FIELDS

# @Optional field : to fill only if fields in header are mapped to tool output (mapping defined by (N) suffix)
#   Specify fields that fill be extracted and placed to csv file
fio_output = (1_READ)READ:bw;(2_WRITE)WRITE:bw;(3_ios_r)dm-0:ios:1['/'];(4_ios_w)dm-0:ios:2['/']
#;(5)cpu:usr;(6)cpu:sys;(7)cpu:ctx

# @Optional field : to fill only if fields in header are mapped to tool output (mapping defined by (N) suffix)
#   Because vmstat is array type file each field is a column of values,
#   here we need to transform it to single value.
#   [(*1), *2] describe transformation:
#       - *1 describe rows used to calculate: i.e. get all rows: (0:)=(0:-1)=(:), get all rows but first (1:)=(1:-1)gTgT
#       - *2 describe function used: currently supported 'avg' - avarage, 'med' - median
vmstat_fields = (5)cs[(1:),med];(6)us[(1:),med];(7)sy[(1:),med];(8)id[(1:),med];(9)wa[(1:),med]
#vmstat_fields = (8)cs[(1:),med];(9)us[(1:),med];(10)sy[(1:),med];(11)id[(1:),med];(12)wa[(1:),med]
iostat_fields =
mpstat_fields =
sar_fields =
perf_fields =
